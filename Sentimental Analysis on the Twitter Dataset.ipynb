{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367379bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eab857",
   "metadata": {},
   "source": [
    "## 1. Load the provided CSV file “Sentiment.csv” and process this file as needed to handle text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ead87bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6dd47249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_confidence</th>\n",
       "      <th>relevant_yn</th>\n",
       "      <th>relevant_yn_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>subject_matter</th>\n",
       "      <th>subject_matter_confidence</th>\n",
       "      <th>candidate_gold</th>\n",
       "      <th>...</th>\n",
       "      <th>relevant_yn_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>subject_matter_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697200650592256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199560069120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199312482304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697197118861312</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697196967903232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               candidate  candidate_confidence relevant_yn  \\\n",
       "0   1  No candidate mentioned                   1.0         yes   \n",
       "1   2            Scott Walker                   1.0         yes   \n",
       "2   3  No candidate mentioned                   1.0         yes   \n",
       "3   4  No candidate mentioned                   1.0         yes   \n",
       "4   5            Donald Trump                   1.0         yes   \n",
       "\n",
       "   relevant_yn_confidence sentiment  sentiment_confidence     subject_matter  \\\n",
       "0                     1.0   Neutral                0.6578  None of the above   \n",
       "1                     1.0  Positive                0.6333  None of the above   \n",
       "2                     1.0   Neutral                0.6629  None of the above   \n",
       "3                     1.0  Positive                1.0000  None of the above   \n",
       "4                     1.0  Positive                0.7045  None of the above   \n",
       "\n",
       "   subject_matter_confidence candidate_gold  ... relevant_yn_gold  \\\n",
       "0                     1.0000            NaN  ...              NaN   \n",
       "1                     1.0000            NaN  ...              NaN   \n",
       "2                     0.6629            NaN  ...              NaN   \n",
       "3                     0.7039            NaN  ...              NaN   \n",
       "4                     1.0000            NaN  ...              NaN   \n",
       "\n",
       "  retweet_count  sentiment_gold subject_matter_gold  \\\n",
       "0             5             NaN                 NaN   \n",
       "1            26             NaN                 NaN   \n",
       "2            27             NaN                 NaN   \n",
       "3           138             NaN                 NaN   \n",
       "4           156             NaN                 NaN   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         NaN   \n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
       "\n",
       "               tweet_created            tweet_id  tweet_location  \\\n",
       "0  2015-08-07 09:54:46 -0700  629697200650592256             NaN   \n",
       "1  2015-08-07 09:54:46 -0700  629697199560069120             NaN   \n",
       "2  2015-08-07 09:54:46 -0700  629697199312482304             NaN   \n",
       "3  2015-08-07 09:54:45 -0700  629697197118861312           Texas   \n",
       "4  2015-08-07 09:54:45 -0700  629697196967903232             NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                       Quito  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Central Time (US & Canada)  \n",
       "4                     Arizona  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff072feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selecting the text and sentiment columns for further analysis\n",
    "\n",
    "dataset = dataset[['text','sentiment']] \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1d5789a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        rt @nancyleegrahn: how did everyone feel about...\n",
      "1        rt @scottwalker: didn't catch the full #gopdeb...\n",
      "2        rt @tjmshow: no mention of tamir rice and the ...\n",
      "3        rt @robgeorge: that carly fiorina is trending ...\n",
      "4        rt @danscavino: #gopdebate w/ @realdonaldtrump...\n",
      "                               ...                        \n",
      "13866    rt @cappy_yarbrough: love to see men who will ...\n",
      "13867    rt @georgehenryw: who thought huckabee exceede...\n",
      "13868    rt @lrihendry: #tedcruz as president, i will a...\n",
      "13869    rt @jrehling: #gopdebate donald trump says tha...\n",
      "13870    rt @lrihendry: #tedcruz headed into the presid...\n",
      "Name: text, Length: 13871, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Using lambda expression to convert string to lowercase\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: x.lower()) \n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b38ddb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        rt nancyleegrahn how did everyone feel about t...\n",
      "1        rt scottwalker didnt catch the full gopdebate ...\n",
      "2        rt tjmshow no mention of tamir rice and the go...\n",
      "3        rt robgeorge that carly fiorina is trending  h...\n",
      "4        rt danscavino gopdebate w realdonaldtrump deli...\n",
      "                               ...                        \n",
      "13866    rt cappy_yarbrough love to see men who will ne...\n",
      "13867    rt georgehenryw who thought huckabee exceeded ...\n",
      "13868    rt lrihendry tedcruz as president i will alway...\n",
      "13869    rt jrehling gopdebate donald trump says that h...\n",
      "13870    rt lrihendry tedcruz headed into the president...\n",
      "Name: text, Length: 13871, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# importing regular expression library\n",
    "\n",
    "import re\n",
    "\n",
    "dataset['text'] = dataset['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) \n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "909e44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          nancyleegrahn how did everyone feel about th...\n",
      "1          scottwalker didnt catch the full gopdebate l...\n",
      "2          tjmshow no mention of tamir rice and the gop...\n",
      "3          robgeorge that carly fiorina is trending  ho...\n",
      "4          danscavino gopdebate w realdonaldtrump deliv...\n",
      "                               ...                        \n",
      "13866      cappy_yarbrough love to see men who will nev...\n",
      "13867      georgehenryw who thought huckabee exceeded t...\n",
      "13868      lrihendry tedcruz as president i will always...\n",
      "13869      jrehling gopdebate donald trump says that he...\n",
      "13870      lrihendry tedcruz headed into the presidenti...\n",
      "Name: text, Length: 13871, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Itering over the data frame \n",
    "# using dataset.iterrows()\n",
    "\n",
    "for idx, row in dataset.iterrows(): \n",
    "\trow[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d2580",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a191592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      nancyleegrahn how did everyone feel about th...\n",
       "1      scottwalker didnt catch the full gopdebate l...\n",
       "2      tjmshow no mention of tamir rice and the gop...\n",
       "3      robgeorge that carly fiorina is trending  ho...\n",
       "4      danscavino gopdebate w realdonaldtrump deliv...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing punctuations\n",
    "\n",
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "dataset['text']= dataset['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96a37b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13866      cappyyarbrough love to see men who will neve...\n",
       "13867      georgehenryw who thought huckabee exceeded t...\n",
       "13868      lrihendry tedcruz as president i will always...\n",
       "13869      jrehling gopdebate donald trump says that he...\n",
       "13870      lrihendry tedcruz headed into the presidenti...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing repeating characters\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d97cc305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13866      cappyyarbrough love to see men who will neve...\n",
       "13867      georgehenryw who thought huckabee exceeded t...\n",
       "13868      lrihendry tedcruz as president i will always...\n",
       "13869      jrehling gopdebate donald trump says that he...\n",
       "13870      lrihendry tedcruz headed into the presidenti...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing URL’s\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "78455a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    nancyleegrahn how did everyone feel about the ...\n",
       "1    scottwalker didnt catch the full gopdebate las...\n",
       "2    tjmshow no mention of tamir rice and the gopde...\n",
       "3    robgeorge that carly fiorina is trending hours...\n",
       "4    danscavino gopdebate w realdonaldtrump deliver...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split()])\n",
    "dataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad312a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13866    cappyyarbrough love to see men who will never ...\n",
       "13867    georgehenryw who thought huckabee exceeded the...\n",
       "13868    lrihendry tedcruz as president i will always t...\n",
       "13869    jrehling gopdebate donald trump says that he d...\n",
       "13870    lrihendry tedcruz headed into the presidential...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing Numeric numbers\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c2d54c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nancyleegrahn how did everyone feel about the ...\n",
       "1    scottwalker didnt catch the full gopdebate las...\n",
       "2    tjmshow no mention of tamir rice and the gopde...\n",
       "3    robgeorge that carly fiorina is trending hours...\n",
       "4    danscavino gopdebate w realdonaldtrump deliver...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Stemming\n",
    "\n",
    "import nltk\n",
    "st = nltk.LancasterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e5130012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nancyleegrahn how did everyone feel about the ...\n",
       "1    scottwalker didnt catch the full gopdebate las...\n",
       "2    tjmshow no mention of tamir rice and the gopde...\n",
       "3    robgeorge that carly fiorina is trending hours...\n",
       "4    danscavino gopdebate w realdonaldtrump deliver...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Lemmatizer\n",
    "\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13da286",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e39f2a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nancyleegrahn how did everyone feel about the ...\n",
       "1    scottwalker didnt catch the full gopdebate las...\n",
       "2    tjmshow no mention of tamir rice and the gopde...\n",
       "3    robgeorge that carly fiorina is trending hours...\n",
       "4    danscavino gopdebate w realdonaldtrump deliver...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "max_features = 2000 \n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ') \n",
    "tokenizer.fit_on_texts(dataset['text'].values) \n",
    "X = tokenizer.texts_to_sequences(dataset['text'].values) \n",
    "\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "40e55dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (13871, 28)\n"
     ]
    }
   ],
   "source": [
    "# Using pad_sequence to convert reviews into equal length\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X) \n",
    "print('X.shape = ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b401ef3",
   "metadata": {},
   "source": [
    "## 2. Build the Keras model that you have in the PPT use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0721ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 13871, 128)        256000    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 591       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# x.shape is a 2-tuple which represents the shape of x. In this case it is (13871, 28).\n",
    "# x.shape[0] gives the first element in that tuple, which is 13871 (number of rows in an array).\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[0])) \n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(3,activation='Softmax')) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "318fba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 28, 128)           256000    \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 591       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building Keras Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# x.shape[1] gives the second element in the tuple, which is 28 (number of columns in an array).\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1])) \n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(3,activation='Softmax')) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa579acf",
   "metadata": {},
   "source": [
    "## 3. Train and save the model and use the saved model to predict on new text data (ex, “A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c540e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(dataset['sentiment'])\n",
    "y = to_categorical(integer_encoded)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8e2a86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "def createmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1])) \n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) \n",
    "    model.add(Dense(3,activation='Softmax')) \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "699352fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "291/291 - 21s - loss: 0.8241 - accuracy: 0.6461 - 21s/epoch - 74ms/step\n",
      "Epoch 2/5\n",
      "291/291 - 17s - loss: 0.6794 - accuracy: 0.7097 - 17s/epoch - 57ms/step\n",
      "Epoch 3/5\n",
      "291/291 - 16s - loss: 0.6132 - accuracy: 0.7462 - 16s/epoch - 55ms/step\n",
      "Epoch 4/5\n",
      "291/291 - 16s - loss: 0.5693 - accuracy: 0.7626 - 16s/epoch - 56ms/step\n",
      "Epoch 5/5\n",
      "291/291 - 16s - loss: 0.5232 - accuracy: 0.7785 - 16s/epoch - 55ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2840f050c70>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model = createmodel()\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "00c48f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 - 2s - loss: 0.8191 - accuracy: 0.6806 - 2s/epoch - 11ms/step\n",
      "Loss=  0.8191332817077637\n",
      "Accuracy=  0.6806465983390808\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Loss and Accuracy of model\n",
    "\n",
    "score,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size=batch_size)\n",
    "print('Loss= ',score)\n",
    "print('Accuracy= ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef394053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Serialize model to JSON\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    #serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c1aee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load json and create model\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "31d37873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   7 442   5 146 288  35\n",
      "   29  35 367   2 348   8   7 153 264  23]]\n"
     ]
    }
   ],
   "source": [
    "tweet = [\"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\"]\n",
    "\n",
    "# Vectorizing the tweet by the r-fitted tokenizer instance\n",
    "tweet = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "# Padding the tweet to have exactly the same shape as input\n",
    "tweet  =pad_sequences(tweet, maxlen = 28, dtype= 'int32', value = 0)\n",
    "print(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f992f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - 41ms/epoch - 41ms/step\n",
      "[[0.4109609  0.08536554 0.50367355]]\n"
     ]
    }
   ],
   "source": [
    "sentiment = loaded_model.predict(tweet, batch_size=1, verbose=2)\n",
    "print(sentiment)\n",
    "\n",
    "#The conclusion of the sentiment analysis is the tweet is positive with 0.5 and negative with 0.4 and neutral with 0.09."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc0864",
   "metadata": {},
   "source": [
    "## 4. Apply the code on spam data set available in the source code (text classification on the spam.csv data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fbabc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spam dataset\n",
    "\n",
    "data = pd.read_csv(\"spam.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d86a898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4a73e977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point crazy available only in ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry in 2 a wkly comp to win fa cup fina...\n",
       "3          u dun say so early hor u c already then say\n",
       "4    nah i dont think he goes to usf he lives aroun...\n",
       "Name: v2, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting string to lowercase \n",
    "data['v2'] = data['v2'].apply(lambda x: x.lower())\n",
    "\n",
    "# regex to match a string of characters that are not a letters or numbers called regex substitution\n",
    "data['v2'] = data['v2'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "data['v2'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1da9d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in data.iterrows(): \n",
    "\trow[0] = row[0].replace('rt',' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "97c60fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point crazy available only in ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry in 2 a wkly comp to win fa cup fina...\n",
       "3          u dun say so early hor u c already then say\n",
       "4    nah i dont think he goes to usf he lives aroun...\n",
       "5    freemsg hey there darling its been 3 weeks now...\n",
       "6    even my brother is not like to speak with me t...\n",
       "7    as per your request melle melle oru minnaminun...\n",
       "8    winner as a valued network customer you have b...\n",
       "9    had your mobile 11 months or more u r entitled...\n",
       "Name: v2, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 2000 \n",
    "tokenizer = Tokenizer(num_words=max_features, split=\" \") \n",
    "tokenizer.fit_on_texts(data['v2'].values) \n",
    "X = tokenizer.texts_to_sequences(data['v2'].values)\n",
    "data['v2'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b7fccdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   67   58  137]\n",
      " [   0    0    0 ...  443    6 1823]\n",
      " [   0    0    0 ...  459   79  382]\n",
      " ...\n",
      " [   0    0    0 ...   12   19  231]\n",
      " [   0    0    0 ...  198   12   50]\n",
      " [   0    0    0 ...    1   41  258]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a327737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "def spammodel():  \n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1])) \n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) \n",
    "    model.add(Dense(2,activation='Softmax')) \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd50a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_17 (Embedding)    (None, 152, 128)          256000    \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53c38a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3733, 152) (3733, 2)\n",
      "(1839, 152) (1839, 2)\n"
     ]
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(data['v1'])\n",
    "y = to_categorical(integer_encoded)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a58d071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "117/117 - 55s - loss: 0.1631 - accuracy: 0.9429 - 55s/epoch - 473ms/step\n",
      "Epoch 2/5\n",
      "117/117 - 54s - loss: 0.0405 - accuracy: 0.9869 - 54s/epoch - 462ms/step\n",
      "Epoch 3/5\n",
      "117/117 - 59s - loss: 0.0195 - accuracy: 0.9933 - 59s/epoch - 500ms/step\n",
      "Epoch 4/5\n",
      "117/117 - 60s - loss: 0.0109 - accuracy: 0.9965 - 60s/epoch - 511ms/step\n",
      "Epoch 5/5\n",
      "117/117 - 36s - loss: 0.0054 - accuracy: 0.9992 - 36s/epoch - 308ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2841b73dee0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model = spammodel()\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3bfd40e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 - 3s - loss: 0.1039 - accuracy: 0.9837 - 3s/epoch - 46ms/step\n",
      "Loss=  0.10387454926967621\n",
      "Accuracy=  0.9836868047714233\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size=batch_size)\n",
    "print('Loss= ',score)\n",
    "print('Accuracy= ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
